{
  "id": "ml-in-se",
  "title": "Machine Learning in Software Engineering",
  "description": "ML fundamentals for SE, applying ML techniques, and software engineering for ML systems",
  "category": "ml",
  "cards": [
    {
      "question": "How does ML differ from traditional software development?",
      "answer": "Traditional: behavior defined by code. ML: behavior is learned from data; model output is probabilistic and data-dependent.",
      "details": "Slides list three core differences: (1) Data discovery & management; (2) Customization and reuse (models are task/data dependent); (3) Lack of modular development for models. These differences change testing, deployment, and maintenance approaches.",
      "category": "Fundamentals",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 25, "endLine": 35 }
    },
    {
      "question": "What capabilities do ML models commonly provide?",
      "answer": "Capabilities include classification (discrete labels), regression (numeric prediction), ranking/recommender systems, and clustering for finding patterns.",
      "details": "Slides list common ML capabilities (classification, recommendation, clustering) and show that metric selection and modeling choices depend on the capability.",
      "category": "ML Types",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 137, "endLine": 141 }
    },
    {
      "question": "What is the ML development lifecycle?",
      "answer": "Typical pipeline: collect & label data → feature engineering → split data → train/learn model → evaluate; in production: monitor, select data for retraining, and update regularly.",
      "details": "Slides separate static development (data collection, feature engineering, training, evaluation) from production (monitoring, sampling production data for retraining, continuous improvement) and emphasize telemetry.",
      "category": "Process",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 41, "endLine": 76 }
    },
    {
      "question": "What is the role of data in ML systems?",
      "answer": "Data drives model training and quality; model outputs depend on training data characteristics—'garbage in, garbage out'.",
      "details": "Slides stress data discovery and management as a primary difference between ML and traditional development: collecting, cleaning, labeling, and feature preparation are crucial to model success.",
      "category": "Data",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 27, "endLine": 33 }
    },
    {
      "question": "What does evaluation on unseen data tell you and how is it enforced?",
      "answer": "Evaluation on held-out data reveals whether a model generalizes; a large gap between training and evaluation performance indicates overfitting.",
      "details": "Slides recommend keeping a separate evaluation set and using appropriate metrics to detect generalization issues; techniques like cross-validation and modest model complexity help prevent overfitting.",
      "category": "Training",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 60, "endLine": 76 }
    },
    {
      "question": "What is the training/validation/test split and why is it important?",
      "answer": "Split data into training, validation/evaluation, and test sets to avoid data leakage and obtain unbiased estimates of generalization.",
      "details": "Slides state a separate evaluation set is essential; cross-validation and careful splits reduce optimistic estimates and detect overfitting.",
      "category": "Training",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 56, "endLine": 66 }
    },
    {
      "question": "How do you evaluate ML model performance?",
      "answer": "Use task-appropriate metrics: classification uses precision/recall/F1/ROC, regression uses MSE/MAE/R², and ranking tasks use top-K accuracy; match metrics to business goals.",
      "details": "Slides point out that metric choice must fit the problem type and discuss common evaluation metrics and the need to compare learned data vs unseen data to check generalization.",
      "category": "Evaluation",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 68, "endLine": 76 }
    },
    {
      "question": "What is model drift and how do you handle it?",
      "answer": "Model drift: performance degradation due to data or concept drift; the slides recommend monitoring and retraining to maintain accuracy.",
      "details": "Slides list data drift and concept drift as causes of model degradation and recommend monitoring input distributions and model performance and scheduling retraining when needed.",
      "category": "Operations",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 216, "endLine": 255 }
    },
    {
      "question": "What practices support reliable production ML systems?",
      "answer": "Practices include reproducibility (versioning data & models), automated pipelines for training & evaluation, monitoring for drift and performance, and controlled model updates (experiments/rollouts).",
      "details": "Slides emphasize telemetry, model monitoring, retraining, and versioning; they highlight questions about when and how to update models and the need for structured pipelines and monitoring to keep models reliable.",
      "category": "Operations",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 67, "endLine": 140 }
    },
    {
      "question": "Why is consistent feature engineering important?",
      "answer": "Consistent feature computation across training and serving reduces training/serving skew and helps ensure reliable model behavior in production.",
      "details": "Slides emphasize feature engineering practices, data cleaning, and the coupling of feature generation with model training and deployment; consistent transformations are needed for robust production models.",
      "category": "Infrastructure",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 50, "endLine": 56 }
    },
    {
      "question": "How do you test ML systems?",
      "answer": "ML testing requires multiple levels: Data testing (quality, distribution), Model testing (accuracy, fairness, robustness), Infrastructure testing (pipeline, serving). Traditional unit tests still apply to non-ML code.",
      "details": "Data tests: Schema validation, distribution checks, completeness. Model tests: Performance metrics, fairness across groups, adversarial robustness. Pipeline tests: End-to-end runs, reproducibility. Monitoring in production: Performance degradation, data drift. No single test guarantees correctness.",
      "category": "Testing",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 293, "endLine": 360 }
    },
    {
      "question": "What is bias in ML and how do you address it?",
      "answer": "ML bias: Systematic unfairness in model predictions, often affecting protected groups. Sources: Biased training data, proxy variables, sampling bias, labeling bias. Addressing: Diverse data, fairness metrics, bias detection, debiasing techniques.",
      "details": "Examples: Hiring models discriminating by gender, facial recognition performing worse on minorities. Detection: Measure performance across subgroups. Mitigation: Balanced training data, fairness constraints, post-processing. Ethical and legal implications. Fairness definitions often conflict - explicit choices required.",
      "category": "Ethics",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 458, "endLine": 516 }
    },
    {
      "question": "What is the difference between ML models and traditional software for software engineers?",
      "answer": "Traditional: Code defines behavior, deterministic, explicit logic. ML: Data defines behavior, probabilistic, learned patterns. Testing: Unit tests vs statistical evaluation. Debugging: Step through code vs analyze predictions/data.",
      "details": "SE implications: Version control data and models (not just code), monitoring for drift, A/B testing models, handling uncertainty in outputs, explainability requirements. ML systems are software systems - still need: Good architecture, testing, deployment practices. Plus ML-specific practices.",
      "category": "SE Practices",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 1, "endLine": 35 }
    },
    {
      "question": "How are LLMs used in software development?",
      "answer": "Slides discuss that developers use LLMs as tools to assist with tasks like code generation and drafting; LLMs are helpful but have limitations (hallucinations, latency, need for verification).",
      "details": "Slides mention LLMs as developer tools and recommend evaluating risk (error cost, alternatives) and strategies like prompt engineering or verification of outputs.",
      "category": "Applications",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 529, "endLine": 541 }
    },
    {
      "question": "What are key considerations when deploying ML models to production?",
      "answer": "Consider latency, update frequency, locality of execution (client/server/cached), experimentability, monitoring, versioning, and rollback strategies.",
      "details": "Slides discuss tradeoffs for static (in-product) vs client-side vs server-centric vs cached approaches, and recommend designing for monitoring, A/B testing, retraining, and controlled rollouts to mitigate risk.",
      "category": "Deployment",
      "slide_ref": { "file": "wk_13-1_se_ml.md", "startLine": 152, "endLine": 200 }
    }
  ]
}
